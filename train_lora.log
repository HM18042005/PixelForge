2026-01-05 15:20:59,940 - INFO - ================================================================================
2026-01-05 15:20:59,940 - INFO - Starting Stable Diffusion 1.5 LoRA Training
2026-01-05 15:20:59,940 - INFO - ================================================================================
2026-01-05 15:20:59,940 - INFO - Model: runwayml/stable-diffusion-v1-5
2026-01-05 15:20:59,940 - INFO - Resolution: 512x512
2026-01-05 15:20:59,940 - INFO - Batch size: 1
2026-01-05 15:20:59,940 - INFO - Learning rate: 0.0001
2026-01-05 15:20:59,941 - INFO - LoRA rank: 4
2026-01-05 15:20:59,941 - INFO - Epochs: 1
2026-01-05 15:20:59,941 - INFO - Output dir: sd15-lora-output
2026-01-05 15:20:59,941 - INFO - ================================================================================
2026-01-05 15:20:59,941 - INFO - Loading models...
2026-01-05 15:22:27,880 - INFO - ================================================================================
2026-01-05 15:22:27,881 - INFO - Starting Stable Diffusion 1.5 LoRA Training (Demo)
2026-01-05 15:22:27,881 - INFO - ================================================================================
2026-01-05 15:22:27,881 - INFO - Model: runwayml/stable-diffusion-v1-5
2026-01-05 15:22:27,881 - INFO - LoRA rank: 4
2026-01-05 15:22:27,881 - INFO - Learning rate: 0.0001
2026-01-05 15:22:27,881 - INFO - Output dir: sd15-lora-output
2026-01-05 15:22:27,881 - INFO - ================================================================================
2026-01-05 15:22:27,882 - INFO - Loading Stable Diffusion 1.5 models...
2026-01-05 15:22:36,132 - INFO - Freezing base model parameters...
2026-01-05 15:22:36,135 - INFO - Adding LoRA adapters...
2026-01-05 15:22:36,246 - INFO - Setting up optimizer and scheduler...
2026-01-05 15:22:36,249 - INFO - Preparing models with accelerator...
2026-01-05 15:22:43,453 - INFO - Starting training loop...
2026-01-05 15:22:43,453 - INFO - ================================================================================
2026-01-05 15:22:48,641 - INFO - Step 10/50 - Loss: 1.9934
2026-01-05 15:22:53,157 - INFO - Step 20/50 - Loss: 1.9920
2026-01-05 15:22:57,687 - INFO - Step 30/50 - Loss: 1.5957
2026-01-05 15:23:02,213 - INFO - Step 40/50 - Loss: 1.2551
2026-01-05 15:23:06,739 - INFO - Step 50/50 - Loss: 1.4110
2026-01-05 15:23:06,739 - INFO - ================================================================================
2026-01-05 15:23:06,739 - INFO - Saving LoRA weights...
2026-01-05 15:23:06,782 - INFO - ================================================================================
2026-01-05 15:23:06,783 - INFO - ================================================================================
2026-01-05 15:23:06,784 - INFO - 
To use trained LoRA weights:
2026-01-05 15:23:06,784 - INFO -   from diffusers import StableDiffusionPipeline
2026-01-05 15:23:06,784 - INFO -   pipe = StableDiffusionPipeline.from_pretrained('runwayml/stable-diffusion-v1-5')
2026-01-05 15:23:06,784 - INFO -   pipe.load_lora_weights('sd15-lora-output')
2026-01-05 15:23:55,719 - INFO - ================================================================================
2026-01-05 15:23:55,720 - INFO - Starting Stable Diffusion 1.5 LoRA Training (Demo)
2026-01-05 15:23:55,720 - INFO - ================================================================================
2026-01-05 15:23:55,720 - INFO - Model: runwayml/stable-diffusion-v1-5
2026-01-05 15:23:55,720 - INFO - LoRA rank: 4
2026-01-05 15:23:55,720 - INFO - Learning rate: 0.0001
2026-01-05 15:23:55,721 - INFO - Output dir: sd15-lora-output
2026-01-05 15:23:55,721 - INFO - ================================================================================
2026-01-05 15:23:55,721 - INFO - Loading Stable Diffusion 1.5 models...
2026-01-05 15:24:02,660 - INFO - Freezing base model parameters...
2026-01-05 15:24:02,663 - INFO - Adding LoRA adapters...
2026-01-05 15:24:02,767 - INFO - Setting up optimizer and scheduler...
2026-01-05 15:24:02,770 - INFO - Preparing models with accelerator...
2026-01-05 15:24:04,814 - INFO - Starting training loop...
2026-01-05 15:24:04,814 - INFO - ================================================================================
2026-01-05 15:24:09,581 - INFO - Step 10/1800 - Loss: 1.9926
2026-01-05 15:24:13,999 - INFO - Step 20/1800 - Loss: 1.9908
2026-01-05 15:24:18,423 - INFO - Step 30/1800 - Loss: 1.5578
2026-01-05 15:24:22,849 - INFO - Step 40/1800 - Loss: 1.2147
2026-01-05 15:24:27,274 - INFO - Step 50/1800 - Loss: 1.3218
2026-01-05 15:24:31,694 - INFO - Step 60/1800 - Loss: 1.8249
2026-01-05 15:24:36,115 - INFO - Step 70/1800 - Loss: 1.9571
2026-01-05 15:24:40,540 - INFO - Step 80/1800 - Loss: 1.2473
2026-01-05 15:24:44,966 - INFO - Step 90/1800 - Loss: 1.2730
2026-01-05 15:24:49,429 - INFO - Step 100/1800 - Loss: 1.4547
2026-01-05 15:24:53,957 - INFO - Step 110/1800 - Loss: 1.1980
2026-01-05 15:24:58,503 - INFO - Step 120/1800 - Loss: 1.1176
2026-01-05 15:25:03,258 - INFO - Step 130/1800 - Loss: 1.0438
2026-01-05 15:25:08,614 - INFO - Step 140/1800 - Loss: 1.0951
2026-01-05 15:25:13,453 - INFO - Step 150/1800 - Loss: 1.0279
2026-01-05 15:25:18,161 - INFO - Step 160/1800 - Loss: 1.0184
2026-01-05 15:25:22,862 - INFO - Step 170/1800 - Loss: 1.0201
2026-01-05 15:25:27,545 - INFO - Step 180/1800 - Loss: 1.0116
2026-01-05 15:25:32,054 - INFO - Step 190/1800 - Loss: 1.0217
2026-01-05 15:25:36,567 - INFO - Step 200/1800 - Loss: 1.0013
2026-01-05 15:25:41,180 - INFO - Step 210/1800 - Loss: 0.9990
2026-01-05 15:25:45,822 - INFO - Step 220/1800 - Loss: 1.0051
2026-01-05 15:25:50,441 - INFO - Step 230/1800 - Loss: 1.0025
2026-01-05 15:25:55,012 - INFO - Step 240/1800 - Loss: 1.0290
2026-01-05 15:25:59,558 - INFO - Step 250/1800 - Loss: 1.0112
2026-01-05 15:26:04,138 - INFO - Step 260/1800 - Loss: 1.0260
2026-01-05 15:26:08,768 - INFO - Step 270/1800 - Loss: 1.0269
2026-01-05 15:26:13,409 - INFO - Step 280/1800 - Loss: 1.0068
2026-01-05 15:26:18,037 - INFO - Step 290/1800 - Loss: 0.9985
2026-01-05 15:26:22,671 - INFO - Step 300/1800 - Loss: 1.0527
2026-01-05 15:26:27,305 - INFO - Step 310/1800 - Loss: 1.0143
2026-01-05 15:26:31,925 - INFO - Step 320/1800 - Loss: 1.0190
2026-01-05 15:26:36,553 - INFO - Step 330/1800 - Loss: 1.0229
2026-01-05 15:26:41,184 - INFO - Step 340/1800 - Loss: 0.9889
2026-01-05 15:26:45,816 - INFO - Step 350/1800 - Loss: 0.9990
2026-01-05 15:26:50,455 - INFO - Step 360/1800 - Loss: 1.0201
2026-01-05 15:26:55,087 - INFO - Step 370/1800 - Loss: 1.0239
2026-01-05 15:26:59,716 - INFO - Step 380/1800 - Loss: 1.0058
2026-01-05 15:27:04,373 - INFO - Step 390/1800 - Loss: 0.9641
2026-01-05 15:27:09,004 - INFO - Step 400/1800 - Loss: 1.0105
2026-01-05 15:27:13,637 - INFO - Step 410/1800 - Loss: 1.0199
2026-01-05 15:27:18,273 - INFO - Step 420/1800 - Loss: 1.0268
2026-01-05 15:27:22,908 - INFO - Step 430/1800 - Loss: 0.9954
2026-01-05 15:27:27,551 - INFO - Step 440/1800 - Loss: 1.0256
2026-01-05 15:27:32,195 - INFO - Step 450/1800 - Loss: 1.0183
2026-01-05 15:27:36,830 - INFO - Step 460/1800 - Loss: 1.0064
2026-01-05 15:27:41,479 - INFO - Step 470/1800 - Loss: 0.9892
2026-01-05 15:27:46,117 - INFO - Step 480/1800 - Loss: 1.0322
2026-01-05 15:27:50,762 - INFO - Step 490/1800 - Loss: 1.0141
2026-01-05 15:27:55,403 - INFO - Step 500/1800 - Loss: 0.9950
2026-01-05 15:28:00,166 - INFO - Step 510/1800 - Loss: 0.9947
2026-01-05 15:28:04,906 - INFO - Step 520/1800 - Loss: 1.0206
2026-01-05 15:28:09,557 - INFO - Step 530/1800 - Loss: 1.0097
2026-01-05 15:28:14,205 - INFO - Step 540/1800 - Loss: 0.9978
2026-01-05 15:28:18,842 - INFO - Step 550/1800 - Loss: 1.0205
2026-01-05 15:28:23,485 - INFO - Step 560/1800 - Loss: 1.0081
2026-01-05 15:28:28,144 - INFO - Step 570/1800 - Loss: 0.9977
2026-01-05 15:28:32,806 - INFO - Step 580/1800 - Loss: 1.0006
2026-01-05 15:28:37,447 - INFO - Step 590/1800 - Loss: 1.0370
2026-01-05 15:28:42,092 - INFO - Step 600/1800 - Loss: 1.0034
2026-01-05 15:28:46,750 - INFO - Step 610/1800 - Loss: 0.9995
2026-01-05 15:28:51,409 - INFO - Step 620/1800 - Loss: 0.9988
2026-01-05 15:28:56,064 - INFO - Step 630/1800 - Loss: 1.0081
2026-01-05 15:29:00,726 - INFO - Step 640/1800 - Loss: 0.9958
2026-01-05 15:29:06,554 - INFO - Step 650/1800 - Loss: 1.0161
2026-01-05 15:29:11,201 - INFO - Step 660/1800 - Loss: 1.0198
2026-01-05 15:29:15,766 - INFO - Step 670/1800 - Loss: 1.0152
2026-01-05 15:29:20,349 - INFO - Step 680/1800 - Loss: 0.9902
2026-01-05 15:29:24,929 - INFO - Step 690/1800 - Loss: 1.0230
2026-01-05 15:29:29,506 - INFO - Step 700/1800 - Loss: 1.0029
2026-01-05 15:29:34,090 - INFO - Step 710/1800 - Loss: 1.0051
2026-01-05 15:29:38,684 - INFO - Step 720/1800 - Loss: 1.0120
2026-01-05 15:29:43,260 - INFO - Step 730/1800 - Loss: 1.0206
2026-01-05 15:29:47,841 - INFO - Step 740/1800 - Loss: 0.9860
2026-01-05 15:29:52,422 - INFO - Step 750/1800 - Loss: 1.0094
2026-01-05 15:29:57,010 - INFO - Step 760/1800 - Loss: 0.9954
2026-01-05 15:30:01,588 - INFO - Step 770/1800 - Loss: 1.0274
2026-01-05 15:30:06,288 - INFO - Step 780/1800 - Loss: 1.0055
2026-01-05 15:30:10,986 - INFO - Step 790/1800 - Loss: 1.0190
2026-01-05 15:30:15,710 - INFO - Step 800/1800 - Loss: 1.0241
2026-01-05 15:30:20,404 - INFO - Step 810/1800 - Loss: 1.0207
2026-01-05 15:30:25,086 - INFO - Step 820/1800 - Loss: 0.9879
2026-01-05 15:30:29,719 - INFO - Step 830/1800 - Loss: 1.0000
2026-01-05 15:30:34,385 - INFO - Step 840/1800 - Loss: 1.0219
2026-01-05 15:30:39,020 - INFO - Step 850/1800 - Loss: 1.0149
2026-01-05 15:30:43,616 - INFO - Step 860/1800 - Loss: 0.9865
2026-01-05 15:30:48,354 - INFO - Step 870/1800 - Loss: 1.0225
2026-01-05 15:30:53,160 - INFO - Step 880/1800 - Loss: 1.0072
2026-01-05 15:30:57,750 - INFO - Step 890/1800 - Loss: 1.0183
2026-01-05 15:31:02,328 - INFO - Step 900/1800 - Loss: 1.0096
2026-01-05 15:31:06,897 - INFO - Step 910/1800 - Loss: 1.0174
2026-01-05 15:31:11,504 - INFO - Step 920/1800 - Loss: 1.0043
2026-01-05 15:31:16,093 - INFO - Step 930/1800 - Loss: 1.0083
2026-01-05 15:31:20,695 - INFO - Step 940/1800 - Loss: 1.0099
2026-01-05 15:31:25,256 - INFO - Step 950/1800 - Loss: 1.0058
2026-01-05 15:31:29,812 - INFO - Step 960/1800 - Loss: 1.0085
2026-01-05 15:31:34,358 - INFO - Step 970/1800 - Loss: 1.0036
2026-01-05 15:31:38,975 - INFO - Step 980/1800 - Loss: 1.0101
2026-01-05 15:31:43,520 - INFO - Step 990/1800 - Loss: 1.0163
2026-01-05 15:31:48,064 - INFO - Step 1000/1800 - Loss: 0.9903
2026-01-05 15:31:52,642 - INFO - Step 1010/1800 - Loss: 1.0048
2026-01-05 15:31:57,206 - INFO - Step 1020/1800 - Loss: 1.0077
2026-01-05 15:32:01,749 - INFO - Step 1030/1800 - Loss: 1.0086
2026-01-05 15:32:06,291 - INFO - Step 1040/1800 - Loss: 1.0091
2026-01-05 15:32:10,824 - INFO - Step 1050/1800 - Loss: 0.9968
2026-01-05 15:32:15,381 - INFO - Step 1060/1800 - Loss: 1.0211
2026-01-05 15:32:19,884 - INFO - Step 1070/1800 - Loss: 1.0067
2026-01-05 15:32:24,387 - INFO - Step 1080/1800 - Loss: 1.0038
2026-01-05 15:32:28,883 - INFO - Step 1090/1800 - Loss: 0.9978
2026-01-05 15:32:33,373 - INFO - Step 1100/1800 - Loss: 1.0158
2026-01-05 15:32:37,850 - INFO - Step 1110/1800 - Loss: 1.0033
2026-01-05 15:32:42,326 - INFO - Step 1120/1800 - Loss: 1.0146
2026-01-05 15:32:46,809 - INFO - Step 1130/1800 - Loss: 1.0153
2026-01-05 15:32:51,299 - INFO - Step 1140/1800 - Loss: 1.0019
2026-01-05 15:32:55,783 - INFO - Step 1150/1800 - Loss: 0.9797
2026-01-05 15:33:00,267 - INFO - Step 1160/1800 - Loss: 1.0137
2026-01-05 15:33:04,732 - INFO - Step 1170/1800 - Loss: 1.0116
2026-01-05 15:33:09,209 - INFO - Step 1180/1800 - Loss: 1.0067
2026-01-05 15:33:13,696 - INFO - Step 1190/1800 - Loss: 1.0152
2026-01-05 15:33:18,180 - INFO - Step 1200/1800 - Loss: 1.0136
2026-01-05 15:33:22,680 - INFO - Step 1210/1800 - Loss: 1.0097
2026-01-05 15:33:27,214 - INFO - Step 1220/1800 - Loss: 0.9879
2026-01-05 15:33:31,787 - INFO - Step 1230/1800 - Loss: 1.0062
2026-01-05 15:33:36,384 - INFO - Step 1240/1800 - Loss: 1.0013
2026-01-05 15:33:40,939 - INFO - Step 1250/1800 - Loss: 1.0017
2026-01-05 15:33:45,679 - INFO - Step 1260/1800 - Loss: 0.9887
2026-01-05 15:33:50,244 - INFO - Step 1270/1800 - Loss: 1.0091
2026-01-05 15:33:54,802 - INFO - Step 1280/1800 - Loss: 0.9717
2026-01-05 15:33:59,362 - INFO - Step 1290/1800 - Loss: 1.0026
2026-01-05 15:34:03,930 - INFO - Step 1300/1800 - Loss: 1.0120
2026-01-05 15:34:08,536 - INFO - Step 1310/1800 - Loss: 1.0052
2026-01-05 15:34:13,109 - INFO - Step 1320/1800 - Loss: 1.0081
2026-01-05 15:34:17,707 - INFO - Step 1330/1800 - Loss: 1.0245
2026-01-05 15:34:22,272 - INFO - Step 1340/1800 - Loss: 0.9975
2026-01-05 15:34:26,806 - INFO - Step 1350/1800 - Loss: 1.0019
2026-01-05 15:34:31,353 - INFO - Step 1360/1800 - Loss: 1.0160
2026-01-05 15:34:35,963 - INFO - Step 1370/1800 - Loss: 1.0016
2026-01-05 15:34:40,521 - INFO - Step 1380/1800 - Loss: 0.9982
2026-01-05 15:34:45,075 - INFO - Step 1390/1800 - Loss: 0.9992
2026-01-05 15:34:49,633 - INFO - Step 1400/1800 - Loss: 1.0167
2026-01-05 15:34:54,187 - INFO - Step 1410/1800 - Loss: 0.9892
2026-01-05 15:34:58,711 - INFO - Step 1420/1800 - Loss: 1.0082
2026-01-05 15:35:03,265 - INFO - Step 1430/1800 - Loss: 1.0163
2026-01-05 15:35:07,773 - INFO - Step 1440/1800 - Loss: 1.0110
2026-01-05 15:35:12,290 - INFO - Step 1450/1800 - Loss: 1.0095
2026-01-05 15:35:16,809 - INFO - Step 1460/1800 - Loss: 0.9976
2026-01-05 15:35:21,339 - INFO - Step 1470/1800 - Loss: 1.0024
2026-01-05 15:35:25,877 - INFO - Step 1480/1800 - Loss: 0.9900
2026-01-05 15:35:30,428 - INFO - Step 1490/1800 - Loss: 1.0283
2026-01-05 15:35:35,019 - INFO - Step 1500/1800 - Loss: 1.0227
2026-01-05 15:35:39,625 - INFO - Step 1510/1800 - Loss: 0.9963
2026-01-05 15:35:44,181 - INFO - Step 1520/1800 - Loss: 0.9987
2026-01-05 15:35:48,762 - INFO - Step 1530/1800 - Loss: 1.0066
2026-01-05 15:35:53,320 - INFO - Step 1540/1800 - Loss: 1.0110
2026-01-05 15:35:57,859 - INFO - Step 1550/1800 - Loss: 1.0082
2026-01-05 15:36:02,418 - INFO - Step 1560/1800 - Loss: 1.0023
2026-01-05 15:36:06,946 - INFO - Step 1570/1800 - Loss: 1.0294
2026-01-05 15:36:11,462 - INFO - Step 1580/1800 - Loss: 1.0089
2026-01-05 15:36:15,970 - INFO - Step 1590/1800 - Loss: 0.9924
2026-01-05 15:36:20,481 - INFO - Step 1600/1800 - Loss: 1.0019
2026-01-05 15:36:24,992 - INFO - Step 1610/1800 - Loss: 0.9978
2026-01-05 15:37:52,067 - INFO - ================================================================================
2026-01-05 15:37:52,067 - INFO - Starting Stable Diffusion 1.5 LoRA Training (Demo)
2026-01-05 15:37:52,067 - INFO - ================================================================================
2026-01-05 15:37:52,067 - INFO - Model: runwayml/stable-diffusion-v1-5
2026-01-05 15:37:52,068 - INFO - LoRA rank: 4
2026-01-05 15:37:52,068 - INFO - Learning rate: 0.0001
2026-01-05 15:37:52,068 - INFO - Output dir: sd15-lora-output
2026-01-05 15:37:52,068 - INFO - ================================================================================
2026-01-05 15:37:52,068 - INFO - Loading Stable Diffusion 1.5 models...
2026-01-05 15:37:57,908 - INFO - Freezing base model parameters...
2026-01-05 15:37:57,911 - INFO - Adding LoRA adapters...
2026-01-05 15:37:58,032 - INFO - Setting up optimizer and scheduler...
2026-01-05 15:37:58,036 - INFO - Preparing models with accelerator...
2026-01-05 15:38:06,164 - INFO - Starting training loop...
2026-01-05 15:38:06,165 - INFO - ================================================================================
2026-01-05 15:38:11,288 - INFO - Step 10/180 - Loss: 1.9928
2026-01-05 15:38:15,754 - INFO - Step 20/180 - Loss: 1.9911
2026-01-05 15:38:20,224 - INFO - Step 30/180 - Loss: 1.5676
2026-01-05 15:38:24,698 - INFO - Step 40/180 - Loss: 1.2246
2026-01-05 15:38:29,176 - INFO - Step 50/180 - Loss: 1.3453
2026-01-05 15:38:33,658 - INFO - Step 60/180 - Loss: 1.8738
2026-01-05 15:38:38,134 - INFO - Step 70/180 - Loss: 1.9761
2026-01-05 15:38:42,624 - INFO - Step 80/180 - Loss: 1.3540
2026-01-05 15:38:47,149 - INFO - Step 90/180 - Loss: 1.5000
2026-01-05 15:38:51,650 - INFO - Step 100/180 - Loss: 1.9026
2026-01-05 15:38:56,142 - INFO - Step 110/180 - Loss: 1.7152
2026-01-05 15:39:00,631 - INFO - Step 120/180 - Loss: 1.6256
2026-01-05 15:39:05,131 - INFO - Step 130/180 - Loss: 1.3054
2026-01-05 15:39:09,622 - INFO - Step 140/180 - Loss: 1.8521
2026-01-05 15:39:14,115 - INFO - Step 150/180 - Loss: 1.1206
2026-01-05 15:39:18,608 - INFO - Step 160/180 - Loss: 1.1145
2026-01-05 15:39:23,109 - INFO - Step 170/180 - Loss: 1.1688
2026-01-05 15:39:27,614 - INFO - Step 180/180 - Loss: 1.3657
2026-01-05 15:39:27,615 - INFO - ================================================================================
2026-01-05 15:39:27,615 - INFO - Saving LoRA weights...
2026-01-05 15:39:27,656 - INFO - ================================================================================
2026-01-05 15:39:27,657 - INFO - ================================================================================
2026-01-05 15:39:27,657 - INFO - 
To use trained LoRA weights:
2026-01-05 15:39:27,657 - INFO -   from diffusers import StableDiffusionPipeline
2026-01-05 15:39:27,658 - INFO -   pipe = StableDiffusionPipeline.from_pretrained('runwayml/stable-diffusion-v1-5')
2026-01-05 15:39:27,658 - INFO -   pipe.load_lora_weights('sd15-lora-output')
2026-01-05 15:43:37,745 - INFO - ================================================================================
2026-01-05 15:43:37,746 - INFO - Starting Stable Diffusion 1.5 LoRA Training (Demo)
2026-01-05 15:43:37,746 - INFO - ================================================================================
2026-01-05 15:43:37,746 - INFO - Model: runwayml/stable-diffusion-v1-5
2026-01-05 15:43:37,746 - INFO - LoRA rank: 4
2026-01-05 15:43:37,746 - INFO - Learning rate: 0.0005
2026-01-05 15:43:37,747 - INFO - Output dir: sd15-lora-output
2026-01-05 15:43:37,747 - INFO - ================================================================================
2026-01-05 15:43:37,747 - INFO - Loading Stable Diffusion 1.5 models...
2026-01-05 15:43:43,830 - INFO - Freezing base model parameters...
2026-01-05 15:43:43,833 - INFO - Adding LoRA adapters...
2026-01-05 15:43:43,939 - INFO - Setting up optimizer and scheduler...
2026-01-05 15:43:43,943 - INFO - Preparing models with accelerator...
2026-01-05 15:43:46,347 - INFO - Starting training loop...
2026-01-05 15:43:46,347 - INFO - ================================================================================
2026-01-05 15:43:51,257 - INFO - Step 10/500 - Loss: 1.9401
2026-01-05 15:43:55,720 - INFO - Step 20/500 - Loss: 1.9222
2026-01-05 15:44:00,179 - INFO - Step 30/500 - Loss: 1.0873
2026-01-05 15:44:04,692 - INFO - Step 40/500 - Loss: 1.0138
2026-01-05 15:44:09,155 - INFO - Step 50/500 - Loss: 1.0118
2026-01-05 15:44:13,621 - INFO - Step 60/500 - Loss: 1.0160
2026-01-05 15:44:18,081 - INFO - Step 70/500 - Loss: 1.0173
2026-01-05 15:44:22,538 - INFO - Step 80/500 - Loss: 1.0144
2026-01-05 15:44:27,015 - INFO - Step 90/500 - Loss: 1.0037
2026-01-05 15:44:31,495 - INFO - Step 100/500 - Loss: 1.0256
2026-01-05 15:44:35,987 - INFO - Step 110/500 - Loss: 1.0175
2026-01-05 15:44:40,536 - INFO - Step 120/500 - Loss: 1.0234
2026-01-05 15:44:45,004 - INFO - Step 130/500 - Loss: 1.0098
2026-01-05 15:44:49,462 - INFO - Step 140/500 - Loss: 1.0152
2026-01-05 15:44:53,926 - INFO - Step 150/500 - Loss: 1.0169
2026-01-05 15:44:58,386 - INFO - Step 160/500 - Loss: 1.0086
2026-01-05 15:45:02,861 - INFO - Step 170/500 - Loss: 1.0072
2026-01-05 15:45:07,336 - INFO - Step 180/500 - Loss: 0.9858
2026-01-05 15:45:11,801 - INFO - Step 190/500 - Loss: 1.0084
2026-01-05 15:45:16,275 - INFO - Step 200/500 - Loss: 0.9953
2026-01-05 15:45:20,743 - INFO - Step 210/500 - Loss: 0.9905
2026-01-05 15:45:25,216 - INFO - Step 220/500 - Loss: 0.9943
2026-01-05 15:45:29,687 - INFO - Step 230/500 - Loss: 0.9948
2026-01-05 15:45:34,160 - INFO - Step 240/500 - Loss: 1.0021
2026-01-05 15:45:38,642 - INFO - Step 250/500 - Loss: 1.0072
2026-01-05 15:45:43,115 - INFO - Step 260/500 - Loss: 1.0175
2026-01-05 15:45:47,586 - INFO - Step 270/500 - Loss: 1.0080
2026-01-05 15:45:52,060 - INFO - Step 280/500 - Loss: 1.0034
2026-01-05 15:45:56,537 - INFO - Step 290/500 - Loss: 0.9926
2026-01-05 15:46:01,013 - INFO - Step 300/500 - Loss: 1.0206
2026-01-05 15:46:05,486 - INFO - Step 310/500 - Loss: 1.0082
2026-01-05 15:46:09,953 - INFO - Step 320/500 - Loss: 1.0162
2026-01-05 15:46:14,427 - INFO - Step 330/500 - Loss: 1.0085
2026-01-05 15:46:18,902 - INFO - Step 340/500 - Loss: 0.9826
2026-01-05 15:46:23,363 - INFO - Step 350/500 - Loss: 0.9931
2026-01-05 15:46:27,834 - INFO - Step 360/500 - Loss: 1.0097
2026-01-05 15:46:32,308 - INFO - Step 370/500 - Loss: 1.0189
2026-01-05 15:46:36,743 - INFO - Step 380/500 - Loss: 1.0029
2026-01-05 15:46:41,183 - INFO - Step 390/500 - Loss: 0.9614
2026-01-05 15:46:45,629 - INFO - Step 400/500 - Loss: 1.0091
2026-01-05 15:46:50,077 - INFO - Step 410/500 - Loss: 1.0170
2026-01-05 15:46:54,530 - INFO - Step 420/500 - Loss: 1.0145
2026-01-05 15:46:58,989 - INFO - Step 430/500 - Loss: 0.9929
2026-01-05 15:47:03,460 - INFO - Step 440/500 - Loss: 1.0212
2026-01-05 15:47:07,937 - INFO - Step 450/500 - Loss: 1.0032
2026-01-05 15:47:12,418 - INFO - Step 460/500 - Loss: 1.0046
2026-01-05 15:47:16,892 - INFO - Step 470/500 - Loss: 0.9828
2026-01-05 15:47:21,369 - INFO - Step 480/500 - Loss: 1.0194
2026-01-05 15:47:25,843 - INFO - Step 490/500 - Loss: 1.0065
2026-01-05 15:47:30,320 - INFO - Step 500/500 - Loss: 0.9915
2026-01-05 15:47:30,321 - INFO - ================================================================================
2026-01-05 15:47:30,321 - INFO - Saving LoRA weights...
2026-01-05 15:47:30,360 - INFO - ================================================================================
2026-01-05 15:47:30,360 - INFO - ================================================================================
2026-01-05 15:47:30,360 - INFO - 
To use trained LoRA weights:
2026-01-05 15:47:30,360 - INFO -   from diffusers import StableDiffusionPipeline
2026-01-05 15:47:30,360 - INFO -   pipe = StableDiffusionPipeline.from_pretrained('runwayml/stable-diffusion-v1-5')
2026-01-05 15:47:30,361 - INFO -   pipe.load_lora_weights('sd15-lora-output')
2026-01-05 15:51:07,639 - INFO - ================================================================================
2026-01-05 15:51:07,639 - INFO - Starting Stable Diffusion 1.5 LoRA Training
2026-01-05 15:51:07,639 - INFO - ================================================================================
2026-01-05 15:51:07,639 - INFO - Model: runwayml/stable-diffusion-v1-5
2026-01-05 15:51:07,640 - INFO - Resolution: 512x512
2026-01-05 15:51:07,640 - INFO - Batch size: 1
2026-01-05 15:51:07,640 - INFO - Learning rate: 0.0001
2026-01-05 15:51:07,640 - INFO - LoRA rank: 4
2026-01-05 15:51:07,640 - INFO - Epochs: 1
2026-01-05 15:51:07,640 - INFO - Output dir: sd15-lora-output
2026-01-05 15:51:07,640 - INFO - ================================================================================
2026-01-05 15:51:07,641 - INFO - Loading models...
2026-01-05 15:51:07,641 - INFO -   Loading text encoder...
2026-01-05 15:51:09,724 - WARNING - Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
2026-01-05 15:53:06,569 - INFO -   Loading VAE...
2026-01-05 15:53:08,032 - WARNING - Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
2026-01-05 15:54:18,190 - INFO -   Loading UNet...
2026-01-05 15:54:19,946 - WARNING - Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
2026-01-05 16:02:16,000 - INFO -   Loading tokenizer...
2026-01-05 16:02:21,063 - INFO -   Loading scheduler...
2026-01-05 16:02:21,643 - INFO - Freezing base model parameters...
2026-01-05 16:02:21,646 - INFO - Adding LoRA adapters...
2026-01-05 16:02:21,749 - INFO - Trainable parameters: 797,184 / 860,318,148 (0.09%)
2026-01-05 16:02:21,750 - INFO - Loading dataset...
2026-01-05 16:02:21,883 - INFO - Loaded 1800 image-caption pairs
2026-01-05 16:02:21,884 - INFO - Setting up optimizer and scheduler...
2026-01-05 16:02:21,889 - INFO - Preparing with accelerator...
2026-01-05 16:02:24,185 - INFO - Starting training...
2026-01-05 16:02:24,185 - INFO - ================================================================================
2026-01-05 16:02:24,187 - INFO - 
Epoch 1/1
2026-01-05 16:02:35,565 - INFO - Epoch 1, Step 20, Loss: 0.0721
2026-01-05 16:02:45,959 - INFO - Epoch 1, Step 40, Loss: 0.0211
2026-01-05 16:02:56,494 - INFO - Epoch 1, Step 60, Loss: 0.4103
2026-01-05 16:03:06,746 - INFO - Epoch 1, Step 80, Loss: 0.2082
2026-01-05 16:03:16,904 - INFO - Epoch 1, Step 100, Loss: 0.0419
2026-01-05 16:03:27,092 - INFO - Epoch 1, Step 120, Loss: 0.3344
2026-01-05 16:03:37,306 - INFO - Epoch 1, Step 140, Loss: 0.0424
2026-01-05 16:03:47,500 - INFO - Epoch 1, Step 160, Loss: 0.1419
2026-01-05 16:03:57,750 - INFO - Epoch 1, Step 180, Loss: 0.4767
2026-01-05 16:04:07,911 - INFO - Epoch 1, Step 200, Loss: 0.0971
2026-01-05 16:04:07,950 - INFO - Checkpoint saved: sd15-lora-output\checkpoint-200
2026-01-05 16:04:18,165 - INFO - Epoch 1, Step 220, Loss: 0.1054
2026-01-05 16:04:28,570 - INFO - Epoch 1, Step 240, Loss: 0.0252
2026-01-05 16:04:38,839 - INFO - Epoch 1, Step 260, Loss: 0.0073
2026-01-05 16:04:49,151 - INFO - Epoch 1, Step 280, Loss: 0.0872
2026-01-05 16:04:59,554 - INFO - Epoch 1, Step 300, Loss: 0.1133
2026-01-05 16:05:09,754 - INFO - Epoch 1, Step 320, Loss: 0.1633
2026-01-05 16:05:19,981 - INFO - Epoch 1, Step 340, Loss: 0.2101
2026-01-05 16:05:30,235 - INFO - Epoch 1, Step 360, Loss: 0.0366
2026-01-05 16:05:40,416 - INFO - Epoch 1, Step 380, Loss: 0.1211
2026-01-05 16:05:52,015 - INFO - Epoch 1, Step 400, Loss: 0.0385
2026-01-05 16:05:52,071 - INFO - Checkpoint saved: sd15-lora-output\checkpoint-400
2026-01-05 16:06:04,644 - INFO - Epoch 1, Step 420, Loss: 0.1334
2026-01-05 16:06:16,445 - INFO - Epoch 1, Step 440, Loss: 0.0498
2026-01-05 16:06:28,843 - INFO - Epoch 1, Step 460, Loss: 0.0066
2026-01-05 16:06:41,154 - INFO - Epoch 1, Step 480, Loss: 0.0259
2026-01-05 16:06:53,570 - INFO - Epoch 1, Step 500, Loss: 0.2417
2026-01-05 16:07:06,180 - INFO - Epoch 1, Step 520, Loss: 0.0032
2026-01-05 16:07:18,898 - INFO - Epoch 1, Step 540, Loss: 0.0251
2026-01-05 16:07:31,057 - INFO - Epoch 1, Step 560, Loss: 0.0043
2026-01-05 16:07:43,601 - INFO - Epoch 1, Step 580, Loss: 0.2119
2026-01-05 16:07:56,031 - INFO - Epoch 1, Step 600, Loss: 0.0071
2026-01-05 16:07:56,090 - INFO - Checkpoint saved: sd15-lora-output\checkpoint-600
2026-01-05 16:08:08,557 - INFO - Epoch 1, Step 620, Loss: 0.2231
2026-01-05 16:08:20,966 - INFO - Epoch 1, Step 640, Loss: 0.0412
2026-01-05 16:08:33,133 - INFO - Epoch 1, Step 660, Loss: 0.0031
2026-01-05 16:08:43,440 - INFO - Epoch 1, Step 680, Loss: 0.3940
2026-01-05 16:08:53,665 - INFO - Epoch 1, Step 700, Loss: 0.2139
2026-01-05 16:09:03,901 - INFO - Epoch 1, Step 720, Loss: 0.4743
2026-01-05 16:09:14,095 - INFO - Epoch 1, Step 740, Loss: 0.0193
2026-01-05 16:09:24,352 - INFO - Epoch 1, Step 760, Loss: 0.0377
2026-01-05 16:09:34,570 - INFO - Epoch 1, Step 780, Loss: 0.3430
2026-01-05 16:09:44,786 - INFO - Epoch 1, Step 800, Loss: 0.0399
2026-01-05 16:09:44,824 - INFO - Checkpoint saved: sd15-lora-output\checkpoint-800
2026-01-05 16:09:55,129 - INFO - Epoch 1, Step 820, Loss: 0.0841
2026-01-05 16:10:05,390 - INFO - Epoch 1, Step 840, Loss: 0.0098
2026-01-05 16:10:15,645 - INFO - Epoch 1, Step 860, Loss: 0.3896
2026-01-05 16:10:25,839 - INFO - Epoch 1, Step 880, Loss: 0.3512
2026-01-05 16:10:36,070 - INFO - Epoch 1, Step 900, Loss: 0.0965
2026-01-05 16:10:46,273 - INFO - Epoch 1, Step 920, Loss: 0.1365
2026-01-05 16:10:56,516 - INFO - Epoch 1, Step 940, Loss: 0.2014
2026-01-05 16:11:06,747 - INFO - Epoch 1, Step 960, Loss: 0.0063
2026-01-05 16:11:17,174 - INFO - Epoch 1, Step 980, Loss: 0.2818
2026-01-05 16:11:27,423 - INFO - Epoch 1, Step 1000, Loss: 0.0231
2026-01-05 16:11:27,462 - INFO - Checkpoint saved: sd15-lora-output\checkpoint-1000
2026-01-05 16:11:37,813 - INFO - Epoch 1, Step 1020, Loss: 0.0025
2026-01-05 16:11:48,115 - INFO - Epoch 1, Step 1040, Loss: 0.0940
2026-01-05 16:11:58,353 - INFO - Epoch 1, Step 1060, Loss: 0.0388
2026-01-05 16:12:08,717 - INFO - Epoch 1, Step 1080, Loss: 0.1387
2026-01-05 16:12:18,930 - INFO - Epoch 1, Step 1100, Loss: 0.1678
2026-01-05 16:12:29,335 - INFO - Epoch 1, Step 1120, Loss: 0.3680
2026-01-05 16:12:39,587 - INFO - Epoch 1, Step 1140, Loss: 0.0746
2026-01-05 16:12:49,903 - INFO - Epoch 1, Step 1160, Loss: 0.1676
2026-01-05 16:13:00,389 - INFO - Epoch 1, Step 1180, Loss: 0.0063
2026-01-05 16:13:10,528 - INFO - Epoch 1, Step 1200, Loss: 0.0230
2026-01-05 16:13:10,568 - INFO - Checkpoint saved: sd15-lora-output\checkpoint-1200
2026-01-05 16:13:20,863 - INFO - Epoch 1, Step 1220, Loss: 0.0206
2026-01-05 16:13:31,131 - INFO - Epoch 1, Step 1240, Loss: 0.0449
2026-01-05 16:13:41,395 - INFO - Epoch 1, Step 1260, Loss: 0.1384
2026-01-05 16:13:51,693 - INFO - Epoch 1, Step 1280, Loss: 0.2949
2026-01-05 16:14:02,100 - INFO - Epoch 1, Step 1300, Loss: 0.2366
2026-01-05 16:14:12,320 - INFO - Epoch 1, Step 1320, Loss: 0.3654
2026-01-05 16:14:22,610 - INFO - Epoch 1, Step 1340, Loss: 0.0920
2026-01-05 16:14:32,852 - INFO - Epoch 1, Step 1360, Loss: 0.0143
2026-01-05 16:14:43,089 - INFO - Epoch 1, Step 1380, Loss: 0.0164
2026-01-05 16:14:53,284 - INFO - Epoch 1, Step 1400, Loss: 0.0032
2026-01-05 16:14:53,322 - INFO - Checkpoint saved: sd15-lora-output\checkpoint-1400
2026-01-05 16:15:03,778 - INFO - Epoch 1, Step 1420, Loss: 0.0053
2026-01-05 16:15:14,168 - INFO - Epoch 1, Step 1440, Loss: 0.0032
2026-01-05 16:15:24,367 - INFO - Epoch 1, Step 1460, Loss: 0.2661
2026-01-05 16:15:34,864 - INFO - Epoch 1, Step 1480, Loss: 0.0794
2026-01-05 16:15:45,094 - INFO - Epoch 1, Step 1500, Loss: 0.2936
2026-01-05 16:15:55,308 - INFO - Epoch 1, Step 1520, Loss: 0.0102
2026-01-05 16:16:05,758 - INFO - Epoch 1, Step 1540, Loss: 0.0080
2026-01-05 16:16:15,953 - INFO - Epoch 1, Step 1560, Loss: 0.1469
2026-01-05 16:16:26,220 - INFO - Epoch 1, Step 1580, Loss: 0.0172
2026-01-05 16:16:36,604 - INFO - Epoch 1, Step 1600, Loss: 0.1243
2026-01-05 16:16:36,643 - INFO - Checkpoint saved: sd15-lora-output\checkpoint-1600
2026-01-05 16:16:47,101 - INFO - Epoch 1, Step 1620, Loss: 0.0969
2026-01-05 16:16:57,477 - INFO - Epoch 1, Step 1640, Loss: 0.5659
2026-01-05 16:17:07,737 - INFO - Epoch 1, Step 1660, Loss: 0.2070
2026-01-05 16:17:17,980 - INFO - Epoch 1, Step 1680, Loss: 0.0189
2026-01-05 16:17:28,194 - INFO - Epoch 1, Step 1700, Loss: 0.6590
2026-01-05 16:17:38,406 - INFO - Epoch 1, Step 1720, Loss: 0.2269
2026-01-05 16:17:48,627 - INFO - Epoch 1, Step 1740, Loss: 0.3277
2026-01-05 16:17:58,842 - INFO - Epoch 1, Step 1760, Loss: 0.2577
2026-01-05 16:18:09,132 - INFO - Epoch 1, Step 1780, Loss: 0.0403
2026-01-05 16:18:19,284 - INFO - Epoch 1, Step 1800, Loss: 0.4736
2026-01-05 16:18:19,322 - INFO - Checkpoint saved: sd15-lora-output\checkpoint-1800
2026-01-05 16:18:19,324 - INFO - ================================================================================
2026-01-05 16:18:19,324 - INFO - Saving final LoRA weights...
2026-01-05 16:18:19,361 - INFO - LoRA weights saved to sd15-lora-output
2026-01-05 16:18:19,361 - INFO - ================================================================================
2026-01-05 16:18:19,361 - INFO - Training completed successfully!
2026-01-05 16:18:19,361 - INFO - ================================================================================
2026-01-10 15:12:44,079 - INFO - ================================================================================
2026-01-10 15:12:44,079 - INFO - Starting Stable Diffusion 1.5 LoRA Training
2026-01-10 15:12:44,080 - INFO - ================================================================================
2026-01-10 15:12:44,080 - INFO - Model: runwayml/stable-diffusion-v1-5
2026-01-10 15:12:44,080 - INFO - Resolution: 512x512
2026-01-10 15:12:44,081 - INFO - Batch size: 1
2026-01-10 15:12:44,081 - INFO - Learning rate: 0.0001
2026-01-10 15:12:44,081 - INFO - LoRA rank: 4
2026-01-10 15:12:44,081 - INFO - Epochs: 5
2026-01-10 15:12:44,081 - INFO - Output dir: sd15-lora-indoor
2026-01-10 15:12:44,081 - INFO - ================================================================================
2026-01-10 15:12:44,081 - INFO - Loading models...
2026-01-10 15:12:44,081 - INFO -   Loading text encoder...
2026-01-10 15:12:45,572 - INFO -   Loading VAE...
2026-01-10 15:12:46,597 - INFO -   Loading UNet...
2026-01-10 15:12:47,838 - INFO -   Loading tokenizer...
2026-01-10 15:12:49,436 - INFO -   Loading scheduler...
2026-01-10 15:12:49,990 - INFO - Freezing base model parameters...
2026-01-10 15:12:49,994 - INFO - Adding LoRA adapters...
2026-01-10 15:12:50,171 - INFO - Trainable parameters: 797,184 / 860,318,148 (0.09%)
2026-01-10 15:12:50,172 - INFO - Loading dataset...
2026-01-10 15:12:50,316 - INFO - Loaded 1939 image-caption pairs
2026-01-10 15:12:50,317 - INFO - Setting up optimizer and scheduler...
2026-01-10 15:12:50,323 - INFO - Preparing with accelerator...
2026-01-10 15:12:58,881 - INFO - Starting training...
2026-01-10 15:12:58,881 - INFO - ================================================================================
2026-01-10 15:12:58,884 - INFO - 
Epoch 1/5
2026-01-10 15:13:42,316 - INFO - Epoch 1, Step 50, Loss: 0.0447
2026-01-10 15:14:28,315 - INFO - Epoch 1, Step 100, Loss: 0.0386
2026-01-10 15:15:15,632 - INFO - Epoch 1, Step 150, Loss: 0.1497
2026-01-10 15:15:59,850 - INFO - ================================================================================
2026-01-10 15:15:59,850 - INFO - Starting Stable Diffusion 1.5 LoRA Training
2026-01-10 15:15:59,850 - INFO - ================================================================================
2026-01-10 15:15:59,851 - INFO - Model: runwayml/stable-diffusion-v1-5
2026-01-10 15:15:59,851 - INFO - Resolution: 512x512
2026-01-10 15:15:59,852 - INFO - Batch size: 1
2026-01-10 15:15:59,852 - INFO - Learning rate: 0.0001
2026-01-10 15:15:59,852 - INFO - LoRA rank: 4
2026-01-10 15:15:59,852 - INFO - Epochs: 5
2026-01-10 15:15:59,852 - INFO - Output dir: sd15-lora-indoor
2026-01-10 15:15:59,853 - INFO - ================================================================================
2026-01-10 15:15:59,853 - INFO - Loading models...
2026-01-10 15:15:59,853 - INFO -   Loading text encoder...
2026-01-10 15:16:01,204 - INFO -   Loading VAE...
2026-01-10 15:16:02,240 - INFO -   Loading UNet...
2026-01-10 15:16:03,332 - INFO -   Loading tokenizer...
2026-01-10 15:16:04,902 - INFO -   Loading scheduler...
2026-01-10 15:16:05,427 - INFO - Freezing base model parameters...
2026-01-10 15:16:05,429 - INFO - Adding LoRA adapters...
2026-01-10 15:16:05,552 - INFO - Trainable parameters: 797,184 / 860,318,148 (0.09%)
2026-01-10 15:16:05,552 - INFO - Loading dataset...
2026-01-10 15:16:05,670 - INFO - Loaded 1939 image-caption pairs
2026-01-10 15:16:05,670 - INFO - Setting up optimizer and scheduler...
2026-01-10 15:16:05,675 - INFO - Preparing with accelerator...
2026-01-10 15:16:08,113 - INFO - Starting training...
2026-01-10 15:16:08,113 - INFO - ================================================================================
2026-01-10 15:16:08,114 - INFO - 
Epoch 1/5
2026-01-10 15:16:33,604 - INFO - Epoch 1, Step 50, Loss: 0.0447
2026-01-10 15:16:58,260 - INFO - Epoch 1, Step 100, Loss: 0.0386
2026-01-10 15:17:23,074 - INFO - Epoch 1, Step 150, Loss: 0.1497
2026-01-10 15:17:48,341 - INFO - Epoch 1, Step 200, Loss: 0.0985
2026-01-10 15:18:13,961 - INFO - Epoch 1, Step 250, Loss: 0.2426
2026-01-10 15:18:39,686 - INFO - Epoch 1, Step 300, Loss: 0.1082
2026-01-10 15:19:05,349 - INFO - Epoch 1, Step 350, Loss: 0.0028
2026-01-10 15:19:31,262 - INFO - Epoch 1, Step 400, Loss: 0.0555
2026-01-10 15:19:57,229 - INFO - Epoch 1, Step 450, Loss: 0.0717
2026-01-10 15:20:23,412 - INFO - Epoch 1, Step 500, Loss: 0.2405
2026-01-10 15:20:23,453 - INFO - Checkpoint saved: sd15-lora-indoor\checkpoint-500
2026-01-10 15:20:49,701 - INFO - Epoch 1, Step 550, Loss: 0.0198
2026-01-10 15:21:16,306 - INFO - Epoch 1, Step 600, Loss: 0.0082
2026-01-10 15:21:42,810 - INFO - Epoch 1, Step 650, Loss: 0.3453
2026-01-10 15:22:09,240 - INFO - Epoch 1, Step 700, Loss: 0.2339
2026-01-10 15:22:35,680 - INFO - Epoch 1, Step 750, Loss: 0.0502
2026-01-10 15:23:02,444 - INFO - Epoch 1, Step 800, Loss: 0.0434
2026-01-10 15:23:29,464 - INFO - Epoch 1, Step 850, Loss: 0.1966
2026-01-10 15:23:56,339 - INFO - Epoch 1, Step 900, Loss: 0.0622
2026-01-10 15:24:23,613 - INFO - Epoch 1, Step 950, Loss: 0.0882
2026-01-10 15:24:50,816 - INFO - Epoch 1, Step 1000, Loss: 0.0216
2026-01-10 15:24:50,859 - INFO - Checkpoint saved: sd15-lora-indoor\checkpoint-1000
2026-01-10 15:25:18,407 - INFO - Epoch 1, Step 1050, Loss: 0.1529
2026-01-10 15:25:45,872 - INFO - Epoch 1, Step 1100, Loss: 0.1745
2026-01-10 15:26:13,265 - INFO - Epoch 1, Step 1150, Loss: 0.2011
2026-01-10 15:26:41,148 - INFO - Epoch 1, Step 1200, Loss: 0.0220
2026-01-10 15:27:08,696 - INFO - Epoch 1, Step 1250, Loss: 0.0059
2026-01-10 15:27:36,376 - INFO - Epoch 1, Step 1300, Loss: 0.3762
2026-01-10 15:28:03,809 - INFO - Epoch 1, Step 1350, Loss: 0.0386
2026-01-10 15:28:31,486 - INFO - Epoch 1, Step 1400, Loss: 0.0033
2026-01-10 15:28:59,190 - INFO - Epoch 1, Step 1450, Loss: 0.1191
2026-01-10 15:29:26,959 - INFO - Epoch 1, Step 1500, Loss: 0.2516
2026-01-10 15:29:27,000 - INFO - Checkpoint saved: sd15-lora-indoor\checkpoint-1500
2026-01-10 15:29:55,130 - INFO - Epoch 1, Step 1550, Loss: 0.0903
2026-01-10 15:30:23,260 - INFO - Epoch 1, Step 1600, Loss: 0.0954
2026-01-10 15:30:51,396 - INFO - Epoch 1, Step 1650, Loss: 0.4671
2026-01-10 15:31:19,068 - INFO - Epoch 1, Step 1700, Loss: 0.6112
2026-01-10 15:31:46,601 - INFO - Epoch 1, Step 1750, Loss: 0.1128
2026-01-10 15:32:14,679 - INFO - Epoch 1, Step 1800, Loss: 0.5292
2026-01-10 15:32:43,971 - INFO - Epoch 1, Step 1850, Loss: 0.0059
2026-01-10 15:33:13,791 - INFO - Epoch 1, Step 1900, Loss: 0.3149
2026-01-10 15:33:37,001 - INFO - 
Epoch 2/5
2026-01-10 15:33:43,637 - INFO - Epoch 2, Step 1950, Loss: 0.3377
2026-01-10 15:34:12,389 - INFO - Epoch 2, Step 2000, Loss: 0.2834
2026-01-10 15:34:12,432 - INFO - Checkpoint saved: sd15-lora-indoor\checkpoint-2000
2026-01-10 15:34:40,244 - INFO - Epoch 2, Step 2050, Loss: 0.0646
2026-01-10 15:35:08,023 - INFO - Epoch 2, Step 2100, Loss: 0.1386
2026-01-10 15:35:35,906 - INFO - Epoch 2, Step 2150, Loss: 0.0516
2026-01-10 15:36:03,707 - INFO - Epoch 2, Step 2200, Loss: 0.0037
2026-01-10 15:36:31,631 - INFO - Epoch 2, Step 2250, Loss: 0.1812
2026-01-10 15:36:59,504 - INFO - Epoch 2, Step 2300, Loss: 0.3615
2026-01-10 15:37:27,331 - INFO - Epoch 2, Step 2350, Loss: 0.0155
2026-01-10 15:37:55,149 - INFO - Epoch 2, Step 2400, Loss: 0.0639
2026-01-10 15:38:22,762 - INFO - Epoch 2, Step 2450, Loss: 0.2663
2026-01-10 15:38:50,483 - INFO - Epoch 2, Step 2500, Loss: 0.2155
2026-01-10 15:38:50,520 - INFO - Checkpoint saved: sd15-lora-indoor\checkpoint-2500
2026-01-10 15:39:19,042 - INFO - Epoch 2, Step 2550, Loss: 0.1297
2026-01-10 15:39:47,517 - INFO - Epoch 2, Step 2600, Loss: 0.2181
2026-01-10 15:40:16,732 - INFO - Epoch 2, Step 2650, Loss: 0.0147
2026-01-10 15:40:45,884 - INFO - Epoch 2, Step 2700, Loss: 0.0120
2026-01-10 15:41:15,594 - INFO - Epoch 2, Step 2750, Loss: 0.0107
2026-01-10 15:41:43,616 - INFO - Epoch 2, Step 2800, Loss: 0.0322
2026-01-10 15:42:11,456 - INFO - Epoch 2, Step 2850, Loss: 0.1217
2026-01-10 15:42:39,616 - INFO - Epoch 2, Step 2900, Loss: 0.1117
2026-01-10 15:43:09,411 - INFO - Epoch 2, Step 2950, Loss: 0.0150
2026-01-10 15:43:39,494 - INFO - Epoch 2, Step 3000, Loss: 0.1037
2026-01-10 15:43:39,554 - INFO - Checkpoint saved: sd15-lora-indoor\checkpoint-3000
2026-01-10 15:44:09,021 - INFO - Epoch 2, Step 3050, Loss: 0.0075
2026-01-10 15:44:38,749 - INFO - Epoch 2, Step 3100, Loss: 0.0041
2026-01-10 15:45:07,826 - INFO - Epoch 2, Step 3150, Loss: 0.0849
2026-01-10 15:45:34,285 - INFO - Epoch 2, Step 3200, Loss: 0.0210
2026-01-10 15:45:59,783 - INFO - Epoch 2, Step 3250, Loss: 0.0673
2026-01-10 15:46:24,422 - INFO - Epoch 2, Step 3300, Loss: 0.1712
2026-01-10 15:46:48,822 - INFO - Epoch 2, Step 3350, Loss: 0.0772
2026-01-10 15:47:13,159 - INFO - Epoch 2, Step 3400, Loss: 0.1059
2026-01-10 15:47:37,832 - INFO - Epoch 2, Step 3450, Loss: 0.1810
2026-01-10 15:48:02,455 - INFO - Epoch 2, Step 3500, Loss: 0.0910
2026-01-10 15:48:02,493 - INFO - Checkpoint saved: sd15-lora-indoor\checkpoint-3500
2026-01-10 15:48:27,123 - INFO - Epoch 2, Step 3550, Loss: 0.3558
2026-01-10 15:48:51,969 - INFO - Epoch 2, Step 3600, Loss: 0.5495
2026-01-10 15:49:16,801 - INFO - Epoch 2, Step 3650, Loss: 0.0054
2026-01-10 15:49:41,571 - INFO - Epoch 2, Step 3700, Loss: 0.0133
2026-01-10 15:50:06,060 - INFO - Epoch 2, Step 3750, Loss: 0.0395
2026-01-10 15:50:31,109 - INFO - Epoch 2, Step 3800, Loss: 0.1154
2026-01-10 15:50:58,611 - INFO - Epoch 2, Step 3850, Loss: 0.0218
2026-01-10 15:51:12,891 - INFO - 
Epoch 3/5
2026-01-10 15:51:23,703 - INFO - Epoch 3, Step 3900, Loss: 0.2218
2026-01-10 15:51:48,023 - INFO - Epoch 3, Step 3950, Loss: 0.0116
2026-01-10 15:52:13,908 - INFO - Epoch 3, Step 4000, Loss: 0.0207
2026-01-10 15:52:13,955 - INFO - Checkpoint saved: sd15-lora-indoor\checkpoint-4000
2026-01-10 15:52:38,617 - INFO - Epoch 3, Step 4050, Loss: 0.0738
2026-01-10 15:53:03,711 - INFO - Epoch 3, Step 4100, Loss: 0.0029
2026-01-10 15:53:30,114 - INFO - Epoch 3, Step 4150, Loss: 0.2232
2026-01-10 15:53:56,889 - INFO - Epoch 3, Step 4200, Loss: 0.1123
2026-01-10 15:54:23,499 - INFO - Epoch 3, Step 4250, Loss: 0.0824
2026-01-10 15:54:50,899 - INFO - Epoch 3, Step 4300, Loss: 0.1515
2026-01-10 15:55:17,160 - INFO - Epoch 3, Step 4350, Loss: 0.0055
2026-01-10 15:55:41,989 - INFO - Epoch 3, Step 4400, Loss: 0.9677
2026-01-10 15:56:08,045 - INFO - Epoch 3, Step 4450, Loss: 0.0072
2026-01-10 15:56:32,290 - INFO - Epoch 3, Step 4500, Loss: 0.1573
2026-01-10 15:56:32,328 - INFO - Checkpoint saved: sd15-lora-indoor\checkpoint-4500
2026-01-10 15:56:56,755 - INFO - Epoch 3, Step 4550, Loss: 0.0059
2026-01-10 15:57:21,234 - INFO - Epoch 3, Step 4600, Loss: 0.0564
2026-01-10 15:57:45,458 - INFO - Epoch 3, Step 4650, Loss: 0.0551
2026-01-10 15:58:09,847 - INFO - Epoch 3, Step 4700, Loss: 0.2265
2026-01-10 15:58:34,098 - INFO - Epoch 3, Step 4750, Loss: 0.1210
2026-01-10 15:58:58,282 - INFO - Epoch 3, Step 4800, Loss: 0.0045
2026-01-10 15:59:22,689 - INFO - Epoch 3, Step 4850, Loss: 0.0631
2026-01-10 15:59:47,127 - INFO - Epoch 3, Step 4900, Loss: 0.2877
2026-01-10 16:00:11,425 - INFO - Epoch 3, Step 4950, Loss: 0.3267
2026-01-10 16:00:35,778 - INFO - Epoch 3, Step 5000, Loss: 0.0551
2026-01-10 16:00:35,818 - INFO - Checkpoint saved: sd15-lora-indoor\checkpoint-5000
2026-01-10 16:01:00,165 - INFO - Epoch 3, Step 5050, Loss: 0.7241
2026-01-10 16:01:24,333 - INFO - Epoch 3, Step 5100, Loss: 0.0186
2026-01-10 16:01:48,749 - INFO - Epoch 3, Step 5150, Loss: 0.0072
2026-01-10 16:02:13,073 - INFO - Epoch 3, Step 5200, Loss: 0.0039
2026-01-10 16:02:37,244 - INFO - Epoch 3, Step 5250, Loss: 0.0129
2026-01-10 16:03:01,466 - INFO - Epoch 3, Step 5300, Loss: 0.1151
2026-01-10 16:03:25,633 - INFO - Epoch 3, Step 5350, Loss: 0.2716
2026-01-10 16:03:49,826 - INFO - Epoch 3, Step 5400, Loss: 0.0150
2026-01-10 16:04:14,168 - INFO - Epoch 3, Step 5450, Loss: 0.0593
2026-01-10 16:04:38,615 - INFO - Epoch 3, Step 5500, Loss: 0.0091
2026-01-10 16:04:38,650 - INFO - Checkpoint saved: sd15-lora-indoor\checkpoint-5500
2026-01-10 16:05:03,057 - INFO - Epoch 3, Step 5550, Loss: 0.2759
2026-01-10 16:05:27,298 - INFO - Epoch 3, Step 5600, Loss: 0.2878
2026-01-10 16:05:51,521 - INFO - Epoch 3, Step 5650, Loss: 0.2763
2026-01-10 16:06:15,804 - INFO - Epoch 3, Step 5700, Loss: 0.0730
2026-01-10 16:06:40,170 - INFO - Epoch 3, Step 5750, Loss: 0.1196
2026-01-10 16:07:04,327 - INFO - Epoch 3, Step 5800, Loss: 0.3198
2026-01-10 16:07:12,641 - INFO - 
Epoch 4/5
2026-01-10 16:07:28,782 - INFO - Epoch 4, Step 5850, Loss: 0.1874
2026-01-10 16:07:52,949 - INFO - Epoch 4, Step 5900, Loss: 0.0338
2026-01-10 16:08:17,265 - INFO - Epoch 4, Step 5950, Loss: 0.4364
2026-01-10 16:08:41,607 - INFO - Epoch 4, Step 6000, Loss: 0.0853
2026-01-10 16:08:41,646 - INFO - Checkpoint saved: sd15-lora-indoor\checkpoint-6000
2026-01-10 16:09:06,030 - INFO - Epoch 4, Step 6050, Loss: 0.3002
2026-01-10 16:09:30,385 - INFO - Epoch 4, Step 6100, Loss: 0.0035
2026-01-10 16:09:54,671 - INFO - Epoch 4, Step 6150, Loss: 0.0964
2026-01-10 16:10:21,867 - INFO - Epoch 4, Step 6200, Loss: 0.0029
2026-01-10 16:10:49,338 - INFO - Epoch 4, Step 6250, Loss: 0.0793
2026-01-10 16:11:16,719 - INFO - Epoch 4, Step 6300, Loss: 0.1195
2026-01-10 16:11:45,211 - INFO - Epoch 4, Step 6350, Loss: 0.1760
2026-01-10 16:12:17,242 - INFO - Epoch 4, Step 6400, Loss: 0.0957
2026-01-10 16:12:44,606 - INFO - Epoch 4, Step 6450, Loss: 0.0221
2026-01-10 16:13:11,029 - INFO - Epoch 4, Step 6500, Loss: 0.3971
2026-01-10 16:13:11,066 - INFO - Checkpoint saved: sd15-lora-indoor\checkpoint-6500
2026-01-10 16:13:35,156 - INFO - Epoch 4, Step 6550, Loss: 0.0392
2026-01-10 16:13:59,048 - INFO - Epoch 4, Step 6600, Loss: 0.3845
2026-01-10 16:14:22,989 - INFO - Epoch 4, Step 6650, Loss: 0.0104
2026-01-10 16:14:46,905 - INFO - Epoch 4, Step 6700, Loss: 0.1365
2026-01-10 16:15:10,807 - INFO - Epoch 4, Step 6750, Loss: 0.3754
2026-01-10 16:15:34,858 - INFO - Epoch 4, Step 6800, Loss: 0.1941
2026-01-10 16:15:58,916 - INFO - Epoch 4, Step 6850, Loss: 0.0269
2026-01-10 16:16:22,733 - INFO - Epoch 4, Step 6900, Loss: 0.2858
2026-01-10 16:16:46,842 - INFO - Epoch 4, Step 6950, Loss: 0.0557
2026-01-10 16:17:10,864 - INFO - Epoch 4, Step 7000, Loss: 0.4035
2026-01-10 16:17:10,904 - INFO - Checkpoint saved: sd15-lora-indoor\checkpoint-7000
2026-01-10 16:17:34,980 - INFO - Epoch 4, Step 7050, Loss: 0.1058
2026-01-10 16:17:59,568 - INFO - Epoch 4, Step 7100, Loss: 0.0395
2026-01-10 16:18:25,114 - INFO - Epoch 4, Step 7150, Loss: 0.0223
2026-01-10 16:18:50,368 - INFO - Epoch 4, Step 7200, Loss: 0.0130
2026-01-10 16:19:14,615 - INFO - Epoch 4, Step 7250, Loss: 0.1963
2026-01-10 16:19:49,390 - INFO - Epoch 4, Step 7300, Loss: 0.4411
2026-01-10 16:20:29,267 - INFO - Epoch 4, Step 7350, Loss: 0.2256
2026-01-10 16:21:02,494 - INFO - Epoch 4, Step 7400, Loss: 0.0033
2026-01-10 16:21:33,952 - INFO - Epoch 4, Step 7450, Loss: 0.0363
2026-01-10 16:22:16,484 - INFO - Epoch 4, Step 7500, Loss: 0.0111
2026-01-10 16:22:16,540 - INFO - Checkpoint saved: sd15-lora-indoor\checkpoint-7500
2026-01-10 16:22:55,271 - INFO - Epoch 4, Step 7550, Loss: 0.6178
2026-01-10 16:23:39,618 - INFO - Epoch 4, Step 7600, Loss: 0.0685
2026-01-10 16:24:25,778 - INFO - Epoch 4, Step 7650, Loss: 0.0194
2026-01-10 16:25:00,615 - INFO - Epoch 4, Step 7700, Loss: 0.0105
2026-01-10 16:25:35,058 - INFO - Epoch 4, Step 7750, Loss: 0.0931
2026-01-10 16:25:39,704 - INFO - 
Epoch 5/5
2026-01-10 16:26:09,548 - INFO - Epoch 5, Step 7800, Loss: 0.0070
2026-01-10 16:26:43,655 - INFO - Epoch 5, Step 7850, Loss: 0.2205
2026-01-10 16:27:20,805 - INFO - Epoch 5, Step 7900, Loss: 0.0246
2026-01-10 16:27:59,413 - INFO - Epoch 5, Step 7950, Loss: 0.0018
2026-01-10 16:28:32,344 - INFO - Epoch 5, Step 8000, Loss: 0.0114
2026-01-10 16:28:32,381 - INFO - Checkpoint saved: sd15-lora-indoor\checkpoint-8000
2026-01-10 16:29:09,784 - INFO - Epoch 5, Step 8050, Loss: 0.2140
2026-01-10 16:29:46,669 - INFO - Epoch 5, Step 8100, Loss: 0.0088
2026-01-10 16:30:23,540 - INFO - Epoch 5, Step 8150, Loss: 0.0795
2026-01-10 16:31:00,501 - INFO - Epoch 5, Step 8200, Loss: 0.0163
2026-01-10 16:31:36,717 - INFO - Epoch 5, Step 8250, Loss: 0.4062
2026-01-10 16:32:12,499 - INFO - Epoch 5, Step 8300, Loss: 0.0419
2026-01-10 16:32:48,535 - INFO - Epoch 5, Step 8350, Loss: 0.4077
2026-01-10 16:33:24,907 - INFO - Epoch 5, Step 8400, Loss: 0.0533
2026-01-10 16:34:02,896 - INFO - Epoch 5, Step 8450, Loss: 0.0156
2026-01-10 16:34:37,509 - INFO - Epoch 5, Step 8500, Loss: 0.0336
2026-01-10 16:34:37,544 - INFO - Checkpoint saved: sd15-lora-indoor\checkpoint-8500
2026-01-10 16:35:15,341 - INFO - Epoch 5, Step 8550, Loss: 0.0711
2026-01-10 16:35:50,138 - INFO - Epoch 5, Step 8600, Loss: 0.1027
2026-01-10 16:36:23,729 - INFO - Epoch 5, Step 8650, Loss: 0.0952
2026-01-10 16:37:03,699 - INFO - Epoch 5, Step 8700, Loss: 0.5565
2026-01-10 16:37:41,744 - INFO - Epoch 5, Step 8750, Loss: 0.1238
2026-01-10 16:38:19,186 - INFO - Epoch 5, Step 8800, Loss: 0.0176
2026-01-10 16:38:56,813 - INFO - Epoch 5, Step 8850, Loss: 0.0037
2026-01-10 16:39:32,019 - INFO - Epoch 5, Step 8900, Loss: 0.2291
2026-01-10 16:40:08,376 - INFO - Epoch 5, Step 8950, Loss: 0.2748
2026-01-10 16:40:44,075 - INFO - Epoch 5, Step 9000, Loss: 0.0049
2026-01-10 16:40:44,115 - INFO - Checkpoint saved: sd15-lora-indoor\checkpoint-9000
2026-01-10 16:41:17,649 - INFO - Epoch 5, Step 9050, Loss: 0.3198
2026-01-10 16:41:54,780 - INFO - Epoch 5, Step 9100, Loss: 0.1691
2026-01-10 16:42:29,890 - INFO - Epoch 5, Step 9150, Loss: 0.1973
2026-01-10 16:43:05,776 - INFO - Epoch 5, Step 9200, Loss: 0.0208
2026-01-10 16:43:44,102 - INFO - Epoch 5, Step 9250, Loss: 0.0647
2026-01-10 16:44:11,280 - INFO - Epoch 5, Step 9300, Loss: 0.0096
2026-01-10 16:44:35,643 - INFO - Epoch 5, Step 9350, Loss: 0.1927
2026-01-10 16:44:59,701 - INFO - Epoch 5, Step 9400, Loss: 0.0150
2026-01-10 16:45:23,768 - INFO - Epoch 5, Step 9450, Loss: 0.0030
2026-01-10 16:45:48,164 - INFO - Epoch 5, Step 9500, Loss: 0.0337
2026-01-10 16:45:48,203 - INFO - Checkpoint saved: sd15-lora-indoor\checkpoint-9500
2026-01-10 16:46:12,321 - INFO - Epoch 5, Step 9550, Loss: 0.0157
2026-01-10 16:46:36,502 - INFO - Epoch 5, Step 9600, Loss: 0.0175
2026-01-10 16:47:00,689 - INFO - Epoch 5, Step 9650, Loss: 0.2227
2026-01-10 16:47:22,353 - INFO - ================================================================================
2026-01-10 16:47:22,353 - INFO - Saving final LoRA weights...
2026-01-10 16:47:22,395 - INFO - LoRA weights saved to sd15-lora-indoor
2026-01-10 16:47:22,395 - INFO - ================================================================================
2026-01-10 16:47:22,395 - INFO - Training completed successfully!
2026-01-10 16:47:22,395 - INFO - ================================================================================
