2026-01-05 15:20:59,940 - INFO - ================================================================================
2026-01-05 15:20:59,940 - INFO - Starting Stable Diffusion 1.5 LoRA Training
2026-01-05 15:20:59,940 - INFO - ================================================================================
2026-01-05 15:20:59,940 - INFO - Model: runwayml/stable-diffusion-v1-5
2026-01-05 15:20:59,940 - INFO - Resolution: 512x512
2026-01-05 15:20:59,940 - INFO - Batch size: 1
2026-01-05 15:20:59,940 - INFO - Learning rate: 0.0001
2026-01-05 15:20:59,941 - INFO - LoRA rank: 4
2026-01-05 15:20:59,941 - INFO - Epochs: 1
2026-01-05 15:20:59,941 - INFO - Output dir: sd15-lora-output
2026-01-05 15:20:59,941 - INFO - ================================================================================
2026-01-05 15:20:59,941 - INFO - Loading models...
2026-01-05 15:22:27,880 - INFO - ================================================================================
2026-01-05 15:22:27,881 - INFO - Starting Stable Diffusion 1.5 LoRA Training (Demo)
2026-01-05 15:22:27,881 - INFO - ================================================================================
2026-01-05 15:22:27,881 - INFO - Model: runwayml/stable-diffusion-v1-5
2026-01-05 15:22:27,881 - INFO - LoRA rank: 4
2026-01-05 15:22:27,881 - INFO - Learning rate: 0.0001
2026-01-05 15:22:27,881 - INFO - Output dir: sd15-lora-output
2026-01-05 15:22:27,881 - INFO - ================================================================================
2026-01-05 15:22:27,882 - INFO - Loading Stable Diffusion 1.5 models...
2026-01-05 15:22:36,132 - INFO - Freezing base model parameters...
2026-01-05 15:22:36,135 - INFO - Adding LoRA adapters...
2026-01-05 15:22:36,246 - INFO - Setting up optimizer and scheduler...
2026-01-05 15:22:36,249 - INFO - Preparing models with accelerator...
2026-01-05 15:22:43,453 - INFO - Starting training loop...
2026-01-05 15:22:43,453 - INFO - ================================================================================
2026-01-05 15:22:48,641 - INFO - Step 10/50 - Loss: 1.9934
2026-01-05 15:22:53,157 - INFO - Step 20/50 - Loss: 1.9920
2026-01-05 15:22:57,687 - INFO - Step 30/50 - Loss: 1.5957
2026-01-05 15:23:02,213 - INFO - Step 40/50 - Loss: 1.2551
2026-01-05 15:23:06,739 - INFO - Step 50/50 - Loss: 1.4110
2026-01-05 15:23:06,739 - INFO - ================================================================================
2026-01-05 15:23:06,739 - INFO - Saving LoRA weights...
2026-01-05 15:23:06,782 - INFO - ================================================================================
2026-01-05 15:23:06,783 - INFO - ================================================================================
2026-01-05 15:23:06,784 - INFO - 
To use trained LoRA weights:
2026-01-05 15:23:06,784 - INFO -   from diffusers import StableDiffusionPipeline
2026-01-05 15:23:06,784 - INFO -   pipe = StableDiffusionPipeline.from_pretrained('runwayml/stable-diffusion-v1-5')
2026-01-05 15:23:06,784 - INFO -   pipe.load_lora_weights('sd15-lora-output')
2026-01-05 15:23:55,719 - INFO - ================================================================================
2026-01-05 15:23:55,720 - INFO - Starting Stable Diffusion 1.5 LoRA Training (Demo)
2026-01-05 15:23:55,720 - INFO - ================================================================================
2026-01-05 15:23:55,720 - INFO - Model: runwayml/stable-diffusion-v1-5
2026-01-05 15:23:55,720 - INFO - LoRA rank: 4
2026-01-05 15:23:55,720 - INFO - Learning rate: 0.0001
2026-01-05 15:23:55,721 - INFO - Output dir: sd15-lora-output
2026-01-05 15:23:55,721 - INFO - ================================================================================
2026-01-05 15:23:55,721 - INFO - Loading Stable Diffusion 1.5 models...
2026-01-05 15:24:02,660 - INFO - Freezing base model parameters...
2026-01-05 15:24:02,663 - INFO - Adding LoRA adapters...
2026-01-05 15:24:02,767 - INFO - Setting up optimizer and scheduler...
2026-01-05 15:24:02,770 - INFO - Preparing models with accelerator...
2026-01-05 15:24:04,814 - INFO - Starting training loop...
2026-01-05 15:24:04,814 - INFO - ================================================================================
2026-01-05 15:24:09,581 - INFO - Step 10/1800 - Loss: 1.9926
2026-01-05 15:24:13,999 - INFO - Step 20/1800 - Loss: 1.9908
2026-01-05 15:24:18,423 - INFO - Step 30/1800 - Loss: 1.5578
2026-01-05 15:24:22,849 - INFO - Step 40/1800 - Loss: 1.2147
2026-01-05 15:24:27,274 - INFO - Step 50/1800 - Loss: 1.3218
2026-01-05 15:24:31,694 - INFO - Step 60/1800 - Loss: 1.8249
2026-01-05 15:24:36,115 - INFO - Step 70/1800 - Loss: 1.9571
2026-01-05 15:24:40,540 - INFO - Step 80/1800 - Loss: 1.2473
2026-01-05 15:24:44,966 - INFO - Step 90/1800 - Loss: 1.2730
2026-01-05 15:24:49,429 - INFO - Step 100/1800 - Loss: 1.4547
2026-01-05 15:24:53,957 - INFO - Step 110/1800 - Loss: 1.1980
2026-01-05 15:24:58,503 - INFO - Step 120/1800 - Loss: 1.1176
2026-01-05 15:25:03,258 - INFO - Step 130/1800 - Loss: 1.0438
2026-01-05 15:25:08,614 - INFO - Step 140/1800 - Loss: 1.0951
2026-01-05 15:25:13,453 - INFO - Step 150/1800 - Loss: 1.0279
2026-01-05 15:25:18,161 - INFO - Step 160/1800 - Loss: 1.0184
2026-01-05 15:25:22,862 - INFO - Step 170/1800 - Loss: 1.0201
2026-01-05 15:25:27,545 - INFO - Step 180/1800 - Loss: 1.0116
2026-01-05 15:25:32,054 - INFO - Step 190/1800 - Loss: 1.0217
2026-01-05 15:25:36,567 - INFO - Step 200/1800 - Loss: 1.0013
2026-01-05 15:25:41,180 - INFO - Step 210/1800 - Loss: 0.9990
2026-01-05 15:25:45,822 - INFO - Step 220/1800 - Loss: 1.0051
2026-01-05 15:25:50,441 - INFO - Step 230/1800 - Loss: 1.0025
2026-01-05 15:25:55,012 - INFO - Step 240/1800 - Loss: 1.0290
2026-01-05 15:25:59,558 - INFO - Step 250/1800 - Loss: 1.0112
2026-01-05 15:26:04,138 - INFO - Step 260/1800 - Loss: 1.0260
2026-01-05 15:26:08,768 - INFO - Step 270/1800 - Loss: 1.0269
2026-01-05 15:26:13,409 - INFO - Step 280/1800 - Loss: 1.0068
2026-01-05 15:26:18,037 - INFO - Step 290/1800 - Loss: 0.9985
2026-01-05 15:26:22,671 - INFO - Step 300/1800 - Loss: 1.0527
2026-01-05 15:26:27,305 - INFO - Step 310/1800 - Loss: 1.0143
2026-01-05 15:26:31,925 - INFO - Step 320/1800 - Loss: 1.0190
2026-01-05 15:26:36,553 - INFO - Step 330/1800 - Loss: 1.0229
2026-01-05 15:26:41,184 - INFO - Step 340/1800 - Loss: 0.9889
2026-01-05 15:26:45,816 - INFO - Step 350/1800 - Loss: 0.9990
2026-01-05 15:26:50,455 - INFO - Step 360/1800 - Loss: 1.0201
2026-01-05 15:26:55,087 - INFO - Step 370/1800 - Loss: 1.0239
2026-01-05 15:26:59,716 - INFO - Step 380/1800 - Loss: 1.0058
2026-01-05 15:27:04,373 - INFO - Step 390/1800 - Loss: 0.9641
2026-01-05 15:27:09,004 - INFO - Step 400/1800 - Loss: 1.0105
2026-01-05 15:27:13,637 - INFO - Step 410/1800 - Loss: 1.0199
2026-01-05 15:27:18,273 - INFO - Step 420/1800 - Loss: 1.0268
2026-01-05 15:27:22,908 - INFO - Step 430/1800 - Loss: 0.9954
2026-01-05 15:27:27,551 - INFO - Step 440/1800 - Loss: 1.0256
2026-01-05 15:27:32,195 - INFO - Step 450/1800 - Loss: 1.0183
2026-01-05 15:27:36,830 - INFO - Step 460/1800 - Loss: 1.0064
2026-01-05 15:27:41,479 - INFO - Step 470/1800 - Loss: 0.9892
2026-01-05 15:27:46,117 - INFO - Step 480/1800 - Loss: 1.0322
2026-01-05 15:27:50,762 - INFO - Step 490/1800 - Loss: 1.0141
2026-01-05 15:27:55,403 - INFO - Step 500/1800 - Loss: 0.9950
2026-01-05 15:28:00,166 - INFO - Step 510/1800 - Loss: 0.9947
2026-01-05 15:28:04,906 - INFO - Step 520/1800 - Loss: 1.0206
2026-01-05 15:28:09,557 - INFO - Step 530/1800 - Loss: 1.0097
2026-01-05 15:28:14,205 - INFO - Step 540/1800 - Loss: 0.9978
2026-01-05 15:28:18,842 - INFO - Step 550/1800 - Loss: 1.0205
2026-01-05 15:28:23,485 - INFO - Step 560/1800 - Loss: 1.0081
2026-01-05 15:28:28,144 - INFO - Step 570/1800 - Loss: 0.9977
2026-01-05 15:28:32,806 - INFO - Step 580/1800 - Loss: 1.0006
2026-01-05 15:28:37,447 - INFO - Step 590/1800 - Loss: 1.0370
2026-01-05 15:28:42,092 - INFO - Step 600/1800 - Loss: 1.0034
2026-01-05 15:28:46,750 - INFO - Step 610/1800 - Loss: 0.9995
2026-01-05 15:28:51,409 - INFO - Step 620/1800 - Loss: 0.9988
2026-01-05 15:28:56,064 - INFO - Step 630/1800 - Loss: 1.0081
2026-01-05 15:29:00,726 - INFO - Step 640/1800 - Loss: 0.9958
2026-01-05 15:29:06,554 - INFO - Step 650/1800 - Loss: 1.0161
2026-01-05 15:29:11,201 - INFO - Step 660/1800 - Loss: 1.0198
2026-01-05 15:29:15,766 - INFO - Step 670/1800 - Loss: 1.0152
2026-01-05 15:29:20,349 - INFO - Step 680/1800 - Loss: 0.9902
2026-01-05 15:29:24,929 - INFO - Step 690/1800 - Loss: 1.0230
2026-01-05 15:29:29,506 - INFO - Step 700/1800 - Loss: 1.0029
2026-01-05 15:29:34,090 - INFO - Step 710/1800 - Loss: 1.0051
2026-01-05 15:29:38,684 - INFO - Step 720/1800 - Loss: 1.0120
2026-01-05 15:29:43,260 - INFO - Step 730/1800 - Loss: 1.0206
2026-01-05 15:29:47,841 - INFO - Step 740/1800 - Loss: 0.9860
2026-01-05 15:29:52,422 - INFO - Step 750/1800 - Loss: 1.0094
2026-01-05 15:29:57,010 - INFO - Step 760/1800 - Loss: 0.9954
2026-01-05 15:30:01,588 - INFO - Step 770/1800 - Loss: 1.0274
2026-01-05 15:30:06,288 - INFO - Step 780/1800 - Loss: 1.0055
2026-01-05 15:30:10,986 - INFO - Step 790/1800 - Loss: 1.0190
2026-01-05 15:30:15,710 - INFO - Step 800/1800 - Loss: 1.0241
2026-01-05 15:30:20,404 - INFO - Step 810/1800 - Loss: 1.0207
2026-01-05 15:30:25,086 - INFO - Step 820/1800 - Loss: 0.9879
2026-01-05 15:30:29,719 - INFO - Step 830/1800 - Loss: 1.0000
2026-01-05 15:30:34,385 - INFO - Step 840/1800 - Loss: 1.0219
2026-01-05 15:30:39,020 - INFO - Step 850/1800 - Loss: 1.0149
2026-01-05 15:30:43,616 - INFO - Step 860/1800 - Loss: 0.9865
2026-01-05 15:30:48,354 - INFO - Step 870/1800 - Loss: 1.0225
2026-01-05 15:30:53,160 - INFO - Step 880/1800 - Loss: 1.0072
2026-01-05 15:30:57,750 - INFO - Step 890/1800 - Loss: 1.0183
2026-01-05 15:31:02,328 - INFO - Step 900/1800 - Loss: 1.0096
2026-01-05 15:31:06,897 - INFO - Step 910/1800 - Loss: 1.0174
2026-01-05 15:31:11,504 - INFO - Step 920/1800 - Loss: 1.0043
2026-01-05 15:31:16,093 - INFO - Step 930/1800 - Loss: 1.0083
2026-01-05 15:31:20,695 - INFO - Step 940/1800 - Loss: 1.0099
2026-01-05 15:31:25,256 - INFO - Step 950/1800 - Loss: 1.0058
2026-01-05 15:31:29,812 - INFO - Step 960/1800 - Loss: 1.0085
2026-01-05 15:31:34,358 - INFO - Step 970/1800 - Loss: 1.0036
2026-01-05 15:31:38,975 - INFO - Step 980/1800 - Loss: 1.0101
2026-01-05 15:31:43,520 - INFO - Step 990/1800 - Loss: 1.0163
2026-01-05 15:31:48,064 - INFO - Step 1000/1800 - Loss: 0.9903
2026-01-05 15:31:52,642 - INFO - Step 1010/1800 - Loss: 1.0048
2026-01-05 15:31:57,206 - INFO - Step 1020/1800 - Loss: 1.0077
2026-01-05 15:32:01,749 - INFO - Step 1030/1800 - Loss: 1.0086
2026-01-05 15:32:06,291 - INFO - Step 1040/1800 - Loss: 1.0091
2026-01-05 15:32:10,824 - INFO - Step 1050/1800 - Loss: 0.9968
2026-01-05 15:32:15,381 - INFO - Step 1060/1800 - Loss: 1.0211
2026-01-05 15:32:19,884 - INFO - Step 1070/1800 - Loss: 1.0067
2026-01-05 15:32:24,387 - INFO - Step 1080/1800 - Loss: 1.0038
2026-01-05 15:32:28,883 - INFO - Step 1090/1800 - Loss: 0.9978
2026-01-05 15:32:33,373 - INFO - Step 1100/1800 - Loss: 1.0158
2026-01-05 15:32:37,850 - INFO - Step 1110/1800 - Loss: 1.0033
2026-01-05 15:32:42,326 - INFO - Step 1120/1800 - Loss: 1.0146
2026-01-05 15:32:46,809 - INFO - Step 1130/1800 - Loss: 1.0153
2026-01-05 15:32:51,299 - INFO - Step 1140/1800 - Loss: 1.0019
2026-01-05 15:32:55,783 - INFO - Step 1150/1800 - Loss: 0.9797
2026-01-05 15:33:00,267 - INFO - Step 1160/1800 - Loss: 1.0137
2026-01-05 15:33:04,732 - INFO - Step 1170/1800 - Loss: 1.0116
2026-01-05 15:33:09,209 - INFO - Step 1180/1800 - Loss: 1.0067
2026-01-05 15:33:13,696 - INFO - Step 1190/1800 - Loss: 1.0152
2026-01-05 15:33:18,180 - INFO - Step 1200/1800 - Loss: 1.0136
2026-01-05 15:33:22,680 - INFO - Step 1210/1800 - Loss: 1.0097
2026-01-05 15:33:27,214 - INFO - Step 1220/1800 - Loss: 0.9879
2026-01-05 15:33:31,787 - INFO - Step 1230/1800 - Loss: 1.0062
2026-01-05 15:33:36,384 - INFO - Step 1240/1800 - Loss: 1.0013
2026-01-05 15:33:40,939 - INFO - Step 1250/1800 - Loss: 1.0017
2026-01-05 15:33:45,679 - INFO - Step 1260/1800 - Loss: 0.9887
2026-01-05 15:33:50,244 - INFO - Step 1270/1800 - Loss: 1.0091
2026-01-05 15:33:54,802 - INFO - Step 1280/1800 - Loss: 0.9717
2026-01-05 15:33:59,362 - INFO - Step 1290/1800 - Loss: 1.0026
2026-01-05 15:34:03,930 - INFO - Step 1300/1800 - Loss: 1.0120
2026-01-05 15:34:08,536 - INFO - Step 1310/1800 - Loss: 1.0052
2026-01-05 15:34:13,109 - INFO - Step 1320/1800 - Loss: 1.0081
2026-01-05 15:34:17,707 - INFO - Step 1330/1800 - Loss: 1.0245
2026-01-05 15:34:22,272 - INFO - Step 1340/1800 - Loss: 0.9975
2026-01-05 15:34:26,806 - INFO - Step 1350/1800 - Loss: 1.0019
2026-01-05 15:34:31,353 - INFO - Step 1360/1800 - Loss: 1.0160
2026-01-05 15:34:35,963 - INFO - Step 1370/1800 - Loss: 1.0016
2026-01-05 15:34:40,521 - INFO - Step 1380/1800 - Loss: 0.9982
2026-01-05 15:34:45,075 - INFO - Step 1390/1800 - Loss: 0.9992
2026-01-05 15:34:49,633 - INFO - Step 1400/1800 - Loss: 1.0167
2026-01-05 15:34:54,187 - INFO - Step 1410/1800 - Loss: 0.9892
2026-01-05 15:34:58,711 - INFO - Step 1420/1800 - Loss: 1.0082
2026-01-05 15:35:03,265 - INFO - Step 1430/1800 - Loss: 1.0163
2026-01-05 15:35:07,773 - INFO - Step 1440/1800 - Loss: 1.0110
2026-01-05 15:35:12,290 - INFO - Step 1450/1800 - Loss: 1.0095
2026-01-05 15:35:16,809 - INFO - Step 1460/1800 - Loss: 0.9976
2026-01-05 15:35:21,339 - INFO - Step 1470/1800 - Loss: 1.0024
2026-01-05 15:35:25,877 - INFO - Step 1480/1800 - Loss: 0.9900
2026-01-05 15:35:30,428 - INFO - Step 1490/1800 - Loss: 1.0283
2026-01-05 15:35:35,019 - INFO - Step 1500/1800 - Loss: 1.0227
2026-01-05 15:35:39,625 - INFO - Step 1510/1800 - Loss: 0.9963
2026-01-05 15:35:44,181 - INFO - Step 1520/1800 - Loss: 0.9987
2026-01-05 15:35:48,762 - INFO - Step 1530/1800 - Loss: 1.0066
2026-01-05 15:35:53,320 - INFO - Step 1540/1800 - Loss: 1.0110
2026-01-05 15:35:57,859 - INFO - Step 1550/1800 - Loss: 1.0082
2026-01-05 15:36:02,418 - INFO - Step 1560/1800 - Loss: 1.0023
2026-01-05 15:36:06,946 - INFO - Step 1570/1800 - Loss: 1.0294
2026-01-05 15:36:11,462 - INFO - Step 1580/1800 - Loss: 1.0089
2026-01-05 15:36:15,970 - INFO - Step 1590/1800 - Loss: 0.9924
2026-01-05 15:36:20,481 - INFO - Step 1600/1800 - Loss: 1.0019
2026-01-05 15:36:24,992 - INFO - Step 1610/1800 - Loss: 0.9978
2026-01-05 15:37:52,067 - INFO - ================================================================================
2026-01-05 15:37:52,067 - INFO - Starting Stable Diffusion 1.5 LoRA Training (Demo)
2026-01-05 15:37:52,067 - INFO - ================================================================================
2026-01-05 15:37:52,067 - INFO - Model: runwayml/stable-diffusion-v1-5
2026-01-05 15:37:52,068 - INFO - LoRA rank: 4
2026-01-05 15:37:52,068 - INFO - Learning rate: 0.0001
2026-01-05 15:37:52,068 - INFO - Output dir: sd15-lora-output
2026-01-05 15:37:52,068 - INFO - ================================================================================
2026-01-05 15:37:52,068 - INFO - Loading Stable Diffusion 1.5 models...
2026-01-05 15:37:57,908 - INFO - Freezing base model parameters...
2026-01-05 15:37:57,911 - INFO - Adding LoRA adapters...
2026-01-05 15:37:58,032 - INFO - Setting up optimizer and scheduler...
2026-01-05 15:37:58,036 - INFO - Preparing models with accelerator...
2026-01-05 15:38:06,164 - INFO - Starting training loop...
2026-01-05 15:38:06,165 - INFO - ================================================================================
2026-01-05 15:38:11,288 - INFO - Step 10/180 - Loss: 1.9928
2026-01-05 15:38:15,754 - INFO - Step 20/180 - Loss: 1.9911
2026-01-05 15:38:20,224 - INFO - Step 30/180 - Loss: 1.5676
2026-01-05 15:38:24,698 - INFO - Step 40/180 - Loss: 1.2246
2026-01-05 15:38:29,176 - INFO - Step 50/180 - Loss: 1.3453
2026-01-05 15:38:33,658 - INFO - Step 60/180 - Loss: 1.8738
2026-01-05 15:38:38,134 - INFO - Step 70/180 - Loss: 1.9761
2026-01-05 15:38:42,624 - INFO - Step 80/180 - Loss: 1.3540
2026-01-05 15:38:47,149 - INFO - Step 90/180 - Loss: 1.5000
2026-01-05 15:38:51,650 - INFO - Step 100/180 - Loss: 1.9026
2026-01-05 15:38:56,142 - INFO - Step 110/180 - Loss: 1.7152
2026-01-05 15:39:00,631 - INFO - Step 120/180 - Loss: 1.6256
2026-01-05 15:39:05,131 - INFO - Step 130/180 - Loss: 1.3054
2026-01-05 15:39:09,622 - INFO - Step 140/180 - Loss: 1.8521
2026-01-05 15:39:14,115 - INFO - Step 150/180 - Loss: 1.1206
2026-01-05 15:39:18,608 - INFO - Step 160/180 - Loss: 1.1145
2026-01-05 15:39:23,109 - INFO - Step 170/180 - Loss: 1.1688
2026-01-05 15:39:27,614 - INFO - Step 180/180 - Loss: 1.3657
2026-01-05 15:39:27,615 - INFO - ================================================================================
2026-01-05 15:39:27,615 - INFO - Saving LoRA weights...
2026-01-05 15:39:27,656 - INFO - ================================================================================
2026-01-05 15:39:27,657 - INFO - ================================================================================
2026-01-05 15:39:27,657 - INFO - 
To use trained LoRA weights:
2026-01-05 15:39:27,657 - INFO -   from diffusers import StableDiffusionPipeline
2026-01-05 15:39:27,658 - INFO -   pipe = StableDiffusionPipeline.from_pretrained('runwayml/stable-diffusion-v1-5')
2026-01-05 15:39:27,658 - INFO -   pipe.load_lora_weights('sd15-lora-output')
